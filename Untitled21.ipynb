{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lsng7tQxelhG",
        "J9y4NiACesSa",
        "SFJloB7Qw5kG"
      ],
      "mount_file_id": "114ymZvMa6hsbVZs_Yy9_ZXVGa7YKyBYl",
      "authorship_tag": "ABX9TyP1dwk6yFZV1duf4aN46px0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codemishka/Prediction_of_Product_Sales/blob/main/Untitled21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of Product Sales\n",
        "> Mishka Janghbahadur\n",
        ">version 6.0 (Project Final Core)"
      ],
      "metadata": {
        "id": "d_wsMMaovDGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA Functions from Lessons"
      ],
      "metadata": {
        "id": "8_z-K9SoeiCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate EDA Functions"
      ],
      "metadata": {
        "id": "lsng7tQxelhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic imports for functions\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# UNIVARIATE PLOTTING FUNCTIONS FOR EDA\n",
        "def explore_categorical(df, x, fillna = True, placeholder = 'MISSING',\n",
        "                        figsize = (6,4), order = None):\n",
        "  \"\"\"Creates a seaborn countplot with the option to temporarily fill missing values\n",
        "  Prints statements about null values, cardinality, and checks for\n",
        "  constant/quasi-constant features.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "  # Make a copy of the dataframe and fillna\n",
        "  temp_df = df.copy()\n",
        "  # Before filling nulls, save null value counts and percent for printing\n",
        "  null_count = temp_df[x].isna().sum()\n",
        "  null_perc = null_count/len(temp_df)* 100\n",
        "  # fillna with placeholder\n",
        "  if fillna == True:\n",
        "    temp_df[x] = temp_df[x].fillna(placeholder)\n",
        "  # Create figure with desired figsize\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  # Plotting a count plot\n",
        "  sns.countplot(data=temp_df, x=x, ax=ax, order=order)\n",
        "  # Rotate Tick Labels for long names\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "  # Add a title with the feature name included\n",
        "  ax.set_title(f\"Column: {x}\", fontweight='bold')\n",
        "\n",
        "  # Fix layout and show plot (before print statements)\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Print null value info\n",
        "  print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "  # Print cardinality info\n",
        "  nunique = temp_df[x].nunique()\n",
        "  print(f\"- Unique Values: {nunique}\")\n",
        "\n",
        "  # First find value counts of feature\n",
        "  val_counts = temp_df[x].value_counts(dropna=False)\n",
        "  # Define the most common value\n",
        "  most_common_val = val_counts.index[0]\n",
        "  # Define the frequency of the most common value\n",
        "  freq = val_counts.values[0]\n",
        "  # Calculate the percentage of the most common value\n",
        "  perc_most_common = freq / len(temp_df) * 100\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"- Most common value: '{most_common_val}' occurs {freq} times ({round(perc_most_common,2)}%)\")\n",
        "  # print message if quasi-constant or constant (most common val more than 98% of data)\n",
        "  if perc_most_common > 98:\n",
        "    print(f\"\\n- [!] Warning: '{x}' is a constant or quasi-constant feature and should be dropped.\")\n",
        "  else:\n",
        "    print(\"- Not constant or quasi-constant.\")\n",
        "  return fig, ax\n",
        "\n",
        "\n",
        "def explore_numeric(df, x, figsize=(6,5) ):\n",
        "  \"\"\"Creates a seaborn histplot and boxplot with a share x-axis,\n",
        "  Prints statements about null values, cardinality, and checks for\n",
        "  constant/quasi-constant features.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "\n",
        "  ## Save null value counts and percent for printing\n",
        "  null_count = df[x].isna().sum()\n",
        "  null_perc = null_count/len(df)* 100\n",
        "\n",
        "\n",
        "  ## Making our figure with gridspec for subplots\n",
        "  gridspec = {'height_ratios':[0.7,0.3]}\n",
        "  fig, axes = plt.subplots(nrows=2, figsize=figsize,\n",
        "                           sharex=True, gridspec_kw=gridspec)\n",
        "  # Histogram on Top\n",
        "  sns.histplot(data=df, x=x, ax=axes[0])\n",
        "\n",
        "  # Boxplot on Bottom\n",
        "  sns.boxplot(data=df, x=x, ax=axes[1])\n",
        "\n",
        "  ## Adding a title\n",
        "  axes[0].set_title(f\"Column: {x}\", fontweight='bold')\n",
        "\n",
        "  ## Adjusting subplots to best fill Figure\n",
        "  fig.tight_layout()\n",
        "\n",
        "  # Ensure plot is shown before message\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Print null value info\n",
        "  print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "  # Print cardinality info\n",
        "  nunique = df[x].nunique()\n",
        "  print(f\"- Unique Values: {nunique}\")\n",
        "\n",
        "\n",
        "  # Get the most most common value, its count as # and as %\n",
        "  most_common_val_count = df[x].value_counts(dropna=False).head(1)\n",
        "  most_common_val = most_common_val_count.index[0]\n",
        "  freq = most_common_val_count.values[0]\n",
        "  perc_most_common = freq / len(df) * 100\n",
        "\n",
        "  print(f\"- Most common value: '{most_common_val}' occurs {freq} times ({round(perc_most_common,2)}%)\")\n",
        "\n",
        "  # print message if quasi-constant or constant (most common val more than 98% of data)\n",
        "  if perc_most_common > 98:\n",
        "    print(f\"\\n- [!] Warning: '{x}' is a constant or quasi-constant feature and should be dropped.\")\n",
        "  else:\n",
        "    print(\"- Not constant or quasi-constant.\")\n",
        "  return fig, axes\n"
      ],
      "metadata": {
        "id": "MpERWZ3we2ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate Feature vs. Target Functions."
      ],
      "metadata": {
        "id": "J9y4NiACesSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"MULTIVARIATE PLOTTING FUNCTIONS VS. NUMERIC TARGET\"\"\"\n",
        "\n",
        "def plot_categorical_vs_target(df, x, y='charges',figsize=(6,4),\n",
        "                            fillna = True, placeholder = 'MISSING',\n",
        "                            order = None):\n",
        "  \"\"\"Plots a combination of a seaborn barplot of means combined with\n",
        "  a seaborn stripplot to show the spread of the data.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "  # Make a copy of the dataframe and fillna\n",
        "  temp_df = df.copy()\n",
        "  # fillna with placeholder\n",
        "  if fillna == True:\n",
        "    temp_df[x] = temp_df[x].fillna(placeholder)\n",
        "\n",
        "  # or drop nulls prevent unwanted 'nan' group in stripplot\n",
        "  else:\n",
        "    temp_df = temp_df.dropna(subset=[x])\n",
        "  # Create the figure and subplots\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Barplot\n",
        "  sns.barplot(data=temp_df, x=x, y=y, ax=ax, order=order, alpha=0.6,\n",
        "              linewidth=1, edgecolor='black', errorbar=None)\n",
        "\n",
        "  # Boxplot\n",
        "  sns.stripplot(data=temp_df, x=x, y=y, hue=x, ax=ax,\n",
        "                order=order, hue_order=order, legend=False,\n",
        "                edgecolor='white', linewidth=0.5,\n",
        "                size=3,zorder=0)\n",
        "  # Rotate xlabels\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "  # Add a title\n",
        "  ax.set_title(f\"{x} vs. {y}\", fontweight='bold')\n",
        "  fig.tight_layout()\n",
        "  return fig, ax\n",
        "\n",
        "\n",
        "def plot_numeric_vs_target(df, x, y='charges',\n",
        "                           figsize=(6,4)):\n",
        "  \"\"\"Plots a seaborn regplot with Pearson's correlation (r) added\n",
        "  to the title.\n",
        "  Source:{PASTE IN FINAL LESSON LINK}\n",
        "  \"\"\"\n",
        "  # Calculate the correlation\n",
        "  corr = df[[x,y]].corr().round(2)\n",
        "  r = corr.loc[x,y]\n",
        "\n",
        "  # Plot the data\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  scatter_kws={'ec':'white','lw':1,'alpha':0.8}\n",
        "  sns.regplot(data=df, x=x, y=y, ax=ax, scatter_kws=scatter_kws)\n",
        "\n",
        "  ## Add the title with the correlation\n",
        "  ax.set_title(f\"{x} vs. {y} (r = {r})\", fontweight='bold')\n",
        "\n",
        "  # Make sure the plot is shown before the print statement\n",
        "  plt.show()\n",
        "\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "Oc90HYFpehRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "py0l2-dbtjtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import missingno as msno\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "bZIM160Z7QaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Typical Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Modeling & preprocessing import\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
        "from sklearn.compose import ColumnTransformer,make_column_transformer,make_column_selector\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder"
      ],
      "metadata": {
        "id": "F2yfr5kcAkcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "SFJloB7Qw5kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bZH8X8Hl_VdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname='/content/drive/MyDrive/CodingDojo/01-Fundamentals/Week02/Data/sales_predictions_2023.csv'\n",
        "df = pd.read_csv(fname)"
      ],
      "metadata": {
        "id": "xwuiNB2vA3c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QhhhyGX3xjtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Inspection"
      ],
      "metadata": {
        "id": "ceBIg9JNx1Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JSqkQMWfxjtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the DataFrame\n",
        "num_rows, num_columns = df.shape\n",
        "\n",
        "# Print using f-strings\n",
        "print(f\"Number of rows: {num_rows}\\nNumber of columns: {num_columns}\")"
      ],
      "metadata": {
        "id": "Muxm_jhdydHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Duplicates and check that no duplicates exist\n",
        "df.drop_duplicates(inplace = True)\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "FHi-AV2my3Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n",
        "#no columns are unnamed"
      ],
      "metadata": {
        "id": "21dwKaJnypUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_types = df.dtypes\n",
        "\n",
        "# Create an empty dictionary to group columns by data type\n",
        "columns_by_type = {}\n",
        "\n",
        "# Group columns by data type\n",
        "for column_name, data_type in data_types.iteritems():\n",
        "    if data_type not in columns_by_type:\n",
        "        columns_by_type[data_type] = [column_name]\n",
        "    else:\n",
        "        columns_by_type[data_type].append(column_name)\n",
        "\n",
        "# Print columns grouped by data type\n",
        "print(\"Columns grouped by data type:\")\n",
        "for data_type, columns in columns_by_type.items():\n",
        "    print(f\"{data_type}: {', '.join(columns)}\")"
      ],
      "metadata": {
        "id": "-dUdE64pzwQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Looking for nulls in each column and convert to percentage\n",
        "null_sums = df.isna().sum()\n",
        "null_sums\n",
        "\n",
        "null_percentage = null_sums/len(df) * 100\n",
        "null_percentage"
      ],
      "metadata": {
        "id": "v3cBT8ML1E8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the % nulls\n",
        "msno.matrix(df);"
      ],
      "metadata": {
        "id": "rSpyl_v036os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "TO2mNjYk2RBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ordinal and Categorical Features"
      ],
      "metadata": {
        "id": "AFfAtJXF2xhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify string columns\n",
        "string_cols = df.select_dtypes(\"object\").columns\n",
        "string_cols\n",
        "\n",
        "# Obtain the value counts for all string columns\n",
        "for col in string_cols:\n",
        "  print(f\"Value Counts for {col}\")\n",
        "  print(df[col].value_counts())\n",
        "  # Increasing readability by adding an empty line\n",
        "  print('\\n')\n"
      ],
      "metadata": {
        "id": "L1lh3-P-2shf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Item_Identifier does not require furthur adjustments\n",
        "# Item_Fat_Content- ordinal values should be either low or regular fat- edit required.\n",
        "# Item_Type does not require furthur adjustments\n",
        "# Outlet_Identifier does not require furthur adjustments\n",
        "# Outlet_Size is ordinal- \"High\" needs to be renamed to \"Large\"-edit required\n",
        "# Outlet_Location_Type is ordinal but doesn't require editing\n",
        "# Outlet_Type- ordinal values should grouped by supermarket type- edit required"
      ],
      "metadata": {
        "id": "VVl8823Z3Jne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning Item_Fat_Content\n",
        "rename_dict = {\"low fat\": \"Low Fat\",\n",
        "               \"LF\": \"Low Fat\",\n",
        "               \"reg\": \"Regular\"}\n",
        "\n",
        "#Item_Fat_Content values are inconsistent, going to standardize with .str.replace\n",
        "#Convert using .str.replace(\"old\",\"new\")\n",
        "df['Item_Fat_Content'] = df['Item_Fat_Content'].replace(rename_dict, regex=True)\n",
        "df['Item_Fat_Content'].value_counts()\n",
        "#We now have standardized our Item_Fat_Content feature"
      ],
      "metadata": {
        "id": "QsYmzefaqf3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting outlet size of \"high\" to \"large\"\n",
        "df[\"Outlet_Size\"].replace({\"High\":\"Large\"}, inplace=True)\n",
        "df[\"Outlet_Size\"].value_counts()"
      ],
      "metadata": {
        "id": "upMJp0HTIyx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning Outlet_Type\n",
        "#Converting outlet size of \"high\" to \"large\"\n",
        "df[\"Outlet_Type\"].replace({\"Grocery Store\":\"Supermarket Type4\"}, inplace=True)\n",
        "df[\"Outlet_Type\"].value_counts()"
      ],
      "metadata": {
        "id": "h-SEqSIf50YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numeric Features"
      ],
      "metadata": {
        "id": "L4ClKYB06hTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the description for numeric features\n",
        "description = df.describe().round(2)\n",
        "description"
      ],
      "metadata": {
        "id": "flVU_7zm6nGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit output to min, max, 25% and 75%\n",
        "description.loc[['min','25%','75%','max']]"
      ],
      "metadata": {
        "id": "MzY3pQME6rxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Address *Item_Outlet_Sales: The max value is much higher than the 75th percentile\n",
        "# CHECK item outlet sales (Item_Outlet_Sales >= 10 000)\n",
        "filter_high_price = df['Item_Outlet_Sales'] >= 10000\n",
        "df[filter_high_price]"
      ],
      "metadata": {
        "id": "PkRl_Vjn6vst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking 5 highest prices to compare  outlier values\n",
        "df.sort_values(\"Item_Outlet_Sales\", ascending=False).head()\n",
        "\n",
        "#It doesn't appear like the max value is an outlier- no need to edit"
      ],
      "metadata": {
        "id": "vkC_iHQt7Jff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualisation\n",
        "\n",
        "Using histograms and boxplots to visualize numeric data"
      ],
      "metadata": {
        "id": "6gfC9ppw7rcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy of dataframe just for visualization and EDA\n",
        "temp_df = df.copy()"
      ],
      "metadata": {
        "id": "2n8SH3-MATCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 'Item_Weight'\n",
        "placeholder = 'MISSING'\n",
        "# In our temporatory df, we will fill in the missing values in the Item_Weight column with the placeholder, MISSING\n",
        "temp_df[x] = temp_df[x].fillna(placeholder)"
      ],
      "metadata": {
        "id": "7YHujtKfcsCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = 'Outlet_Size'\n",
        "placeholder = 'MISSING'\n",
        "# In our temporatory df, we will fill in the missing values in the Outlet_Size column with the placeholder, MISSING\n",
        "temp_df[x] = temp_df[x].fillna(placeholder)"
      ],
      "metadata": {
        "id": "N6cNUl-5c6sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numeric Features"
      ],
      "metadata": {
        "id": "6KdgIh0rD5A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Item_Weight\n",
        "#Item_Visibility\n",
        "#Item_MRP\n",
        "#Outlet_Establishment_Year\n",
        "#Item_Outlet_Sales"
      ],
      "metadata": {
        "id": "wKGjtvydDoOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Histograms to view the distributions of numerical features in your dataset.\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Plot histograms\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, feature in enumerate(numerical_features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    df[feature].hist(bins=30, edgecolor='k')\n",
        "    plt.title(feature)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "j90qj9MfFSqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a custom pastel color palette\n",
        "pastel_palette = sns.color_palette(\"pastel\", len(df.select_dtypes(include=['int64', 'float64']).columns))\n",
        "\n",
        "# Numerical features\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Plot histograms with different pastel colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, feature in enumerate(numerical_features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.histplot(data=df, x=feature, bins=30, edgecolor='k', kde=True, color=pastel_palette[i-1])  # Use the custom color\n",
        "    plt.title(feature)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "ksPIZggvLHVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Seaborn's pastel color palette\n",
        "pastel_palette = sns.color_palette(\"pastel\", len(df.select_dtypes(include=['int64', 'float64']).columns))\n",
        "\n",
        "# Numerical features\n",
        "numerical_features = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year', 'Item_Outlet_Sales']\n",
        "\n",
        "# Creating pastel-colored boxplots for each numerical feature\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(6, 4))  # Adjust the figure size for better visibility\n",
        "    sns.boxplot(data=df[feature], palette=\"pastel\")\n",
        "    plt.title(f'Boxplot of {feature}')\n",
        "    plt.ylabel('Value')\n",
        "    plt.tight_layout()  # Adjust layout\n",
        "    plt.show()\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sh3b4uNSKuh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Seaborn's pastel color palette\n",
        "sns.set_palette(\"pastel\")\n",
        "\n",
        "# Categorical features\n",
        "categorical_features = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
        "\n",
        "# Creating countplots with pastel color palette\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 6))  # Adjust the figure size for better visibility\n",
        "    sns.countplot(data=df, x=feature, palette=\"pastel\")\n",
        "    plt.title(f'Countplot of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "    plt.tight_layout()  # Adjust layout\n",
        "    plt.show()\n",
        "    plt.style.use('dark_background')\n",
        "\n"
      ],
      "metadata": {
        "id": "XE1CF58nJwVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heatmap to view the correlation between features.\n",
        "correlation_matrix = df.corr(numeric_only = True)\n",
        "\n",
        "# Set Seaborn's pastel color palette\n",
        "sns.set_palette(\"pastel\")\n",
        "\n",
        "# Plot the heatmap with pastel colors\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "yywlSjS9F-js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanatory Data Analysis**"
      ],
      "metadata": {
        "id": "NIJVs-nf7tgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the color palette\n",
        "palette = sns.color_palette(\"Set1\", n_colors=len(df['Outlet_Type'].unique()))\n",
        "\n",
        "# Create the scatter plot with regression line\n",
        "sns.lmplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", hue=\"Outlet_Type\", data=df,\n",
        "           scatter_kws={\"edgecolor\": \"white\"}, line_kws={\"color\": \"blue\"},\n",
        "           palette=palette, height=6, aspect=1.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "5X7w9fh4JKqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting each Item Type\n",
        "g = sns.lmplot(df, y=\"Item_Outlet_Sales\", x=\"Item_MRP\", hue=\"Item_Type\",\n",
        "               scatter_kws={\"edgecolor\":\"white\"},aspect=1.5,\n",
        "               col=\"Item_Type\", col_wrap=2)\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "D1SXr5tk7u-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax=sns.histplot(df, x=\"Item_Type\", hue=\"Item_Type\");\n",
        "ax.set_title(\"Types of Products\", fontweight=\"bold\");\n",
        "#Putting $ and , on Sales axis\n",
        "fmt = '${x:,.0f}'\n",
        "tick = mtick.StrMethodFormatter(fmt)\n",
        "ax.yaxis.set_major_formatter(tick)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\");''\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "facW5t8g8qFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lmplot(df, y=\"Item_Outlet_Sales\", x=\"Item_MRP\", hue=\"Item_Type\", scatter_kws={\"edgecolor\":\"black\"});\n",
        "plt.title(\"Sales vs. Item MRP & Item Type\", fontweight=\"bold\");\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "nP1cybZL7qJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = sns.catplot(data=df, y=\"Item_Outlet_Sales\", x=\"Item_Type\", hue=\"Item_Type\")\n",
        "#Putting $ and , on Sales axis\n",
        "fmt = '${x:,.0f}'\n",
        "tick = mtick.StrMethodFormatter(fmt)\n",
        "ax.yaxis.set_major_formatter(tick)\n",
        "g.set_xticklabels(label=\"Item_Type\", rotation=45, ha=\"right\");\n",
        "g.tight_layout();\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "lhgK6Xd6-1_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax= sns.barplot(data=df, y=\"Item_Outlet_Sales\", x=\"Item_Type\");\n",
        "#Putting $ and , on Sales axis\n",
        "fmt = '${x:,.0f}'\n",
        "tick = mtick.StrMethodFormatter(fmt)\n",
        "ax.yaxis.set_major_formatter(tick)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\");\n",
        "ax.grid(ls=\"--\");\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "-fQG9x6NHTI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.boxenplot(data=df, x=\"Item_Outlet_Sales\", y=\"Item_Type\")\n",
        "#Putting $ and , on Sales axis\n",
        "fmt = '${x:,.0f}'\n",
        "tick = mtick.StrMethodFormatter(fmt)\n",
        "ax.xaxis.set_major_formatter(tick)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\");\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "K-DJDV_FM6gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Model"
      ],
      "metadata": {
        "id": "YdCoZH5CdfAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy Path and dataset from scratch\n",
        "\n",
        "fpath=\"/content/drive/MyDrive/CodingDojo/01-Fundamentals/Week02/Data/sales_predictions_2023.csv\"\n",
        "df_copy = pd.read_csv(fpath)\n",
        "df_copy.head()"
      ],
      "metadata": {
        "id": "eNJo6RTGeNWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using information from heatmat correlation, drop unnecessary features.\n",
        "df_copy.drop(columns=[\"Item_Identifier\", \"Item_Weight\", \"Outlet_Establishment_Year\",\n",
        "                  \"Item_Fat_Content\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ZruujYODfY6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSgj26_dmiQI"
      },
      "outputs": [],
      "source": [
        "# Dropping missing values in salary_in_usd column\n",
        "# Must drop these values that are missing since this\n",
        "# column is our target\n",
        "\n",
        "df_copy = df_copy.dropna(subset = ['Item_Outlet_Sales'], how = 'all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVJPexoXND-V"
      },
      "outputs": [],
      "source": [
        "## Define X and y\n",
        "target = 'Item_Outlet_Sales'\n",
        "\n",
        "X = df_copy.drop(columns=target).copy()\n",
        "y = df_copy[target].copy()\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_TO7yiug3aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Duplicates and check that no duplicates exist\n",
        "df_copy.drop_duplicates(inplace = True)\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "RmepZ-AQiil7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting outlet size of \"high\" to \"large\"\n",
        "df_copy[\"Outlet_Size\"].replace({\"High\":\"Large\"}, inplace=True)\n",
        "df_copy[\"Outlet_Size\"].value_counts()"
      ],
      "metadata": {
        "id": "h6krUfrgiimF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning Outlet_Type\n",
        "#Converting outlet size of \"high\" to \"large\"\n",
        "df_copy[\"Outlet_Type\"].replace({\"Grocery Store\":\"Supermarket Type4\"}, inplace=True)\n",
        "df_copy[\"Outlet_Type\"].value_counts()"
      ],
      "metadata": {
        "id": "3ErY89FmiimF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression"
      ],
      "metadata": {
        "id": "Gmz7S1A3_-k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline for Numericals"
      ],
      "metadata": {
        "id": "_47ZqXTjgdz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQXVOMe5OVWI"
      },
      "outputs": [],
      "source": [
        "# Perfoming a train-test-split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(X_train.head(), y_train.head())"
      ],
      "metadata": {
        "id": "gb6PgnRbR97_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_selector = make_column_selector(dtype_include = 'object')\n",
        "cat_selector(X_train)"
      ],
      "metadata": {
        "id": "NQ94d1clUWVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#impute_cat = SimpleImputer(strategy='constant', fill_value = \"Missing\")\n",
        "impute_cat = SimpleImputer(strategy='most_frequent')\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "\n",
        "cat_pipe = make_pipeline(impute_cat,encoder)\n",
        "cat_pipe"
      ],
      "metadata": {
        "id": "rD0zElHUUkWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pipe.fit_transform(X_train[cat_selector(X_train)])"
      ],
      "metadata": {
        "id": "0UkWGXRBVHoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a numeric data selector\n",
        "num_selector = make_column_selector(dtype_include='number')\n",
        "num_selector(X_train)"
      ],
      "metadata": {
        "id": "JgbA6GOXVPX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "Gcfxh6MsVZaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler.fit_transform(X_train[num_selector(X_train)])"
      ],
      "metadata": {
        "id": "4cgFaL2AVfkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = make_column_transformer((cat_pipe,cat_selector),\n",
        "                                       (scaler,num_selector))\n",
        "preprocessor"
      ],
      "metadata": {
        "id": "IYyQUHAPVnZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.head()"
      ],
      "metadata": {
        "id": "wIY31oAPVp6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_cols = df_copy.select_dtypes(\"object\").columns\n",
        "string_cols"
      ],
      "metadata": {
        "id": "Xf3QKXneZC9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in string_cols:\n",
        "  print(f\"Value Counts for {col}\")\n",
        "  print(df_copy[col].value_counts())\n",
        "  # Increasing readability by adding an empty line\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "YQLYWv8_ZRpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.dtypes"
      ],
      "metadata": {
        "id": "0l8bbIF8ZSos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = df_copy.drop(columns = 'Item_Outlet_Sales')\n",
        "y = df_copy['Item_Outlet_Sales']\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n"
      ],
      "metadata": {
        "id": "AL0WS2YFaVXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lE5fisKkkfK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making a Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "VUaWeyxlkiG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_selector = make_column_selector(dtype_include = 'object')\n",
        "cat_selector(X_train)"
      ],
      "metadata": {
        "id": "PUsBDqwDehJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impute_cat =  SimpleImputer(strategy='constant', fill_value = \"Missing\")\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "\n",
        "cat_pipe = make_pipeline(impute_cat,encoder)\n",
        "cat_pipe"
      ],
      "metadata": {
        "id": "jcTe31sjh4k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pipe.fit_transform(X_train[cat_selector(X_train)])"
      ],
      "metadata": {
        "id": "gu4VhmmbjJeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_selector = make_column_selector(dtype_include='number')\n",
        "num_selector(X_train)"
      ],
      "metadata": {
        "id": "ARBkEBzGjY3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "nwEJdAvfjllS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler.fit_transform(X_train[num_selector(X_train)])"
      ],
      "metadata": {
        "id": "0ZoT-gNejobD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = make_column_transformer((cat_pipe,cat_selector),\n",
        "                                       (scaler,num_selector))\n",
        "preprocessor"
      ],
      "metadata": {
        "id": "ZcihE1zUjrJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "rdp-lfgDNFhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "CtNanPO9jzFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "RhNW7hYBfDjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features matrix\n",
        "X = df.drop(columns ='Item_Outlet_Sales')\n",
        "y = df['Item_Outlet_Sales']\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
        "# Preview training data\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "8ICqESK8pi0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom functions for regressoin evaluation\n",
        "def regression_metrics(y_true, y_pred, label='', verbose = True, output_dict=False):\n",
        "  # Get metrics\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  mse = mean_squared_error(y_true, y_pred)\n",
        "  rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "  r_squared = r2_score(y_true, y_pred)\n",
        "  if verbose == True:\n",
        "    # Print Result with Label and Header\n",
        "    header = \"-\"*60\n",
        "    print(header, f\"Regression Metrics: {label}\", header, sep='\\n')\n",
        "    print(f\"- MAE = {mae:,.3f}\")\n",
        "    print(f\"- MSE = {mse:,.3f}\")\n",
        "    print(f\"- RMSE = {rmse:,.3f}\")\n",
        "    print(f\"- R^2 = {r_squared:,.3f}\")\n",
        "  if output_dict == True:\n",
        "      metrics = {'Label':label, 'MAE':mae,\n",
        "                 'MSE':mse, 'RMSE':rmse, 'R^2':r_squared}\n",
        "      return metrics\n",
        "\n",
        "def evaluate_regression(reg, X_train, y_train, X_test, y_test, verbose = True,\n",
        "                        output_frame=False):\n",
        "  # Get predictions for training data\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "\n",
        "  # Call the helper function to obtain regression metrics for training data\n",
        "  results_train = regression_metrics(y_train, y_train_pred, verbose = verbose,\n",
        "                                     output_dict=output_frame,\n",
        "                                     label='Training Data')\n",
        "  print()\n",
        "  # Get predictions for test data\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "  # Call the helper function to obtain regression metrics for test data\n",
        "  results_test = regression_metrics(y_test, y_test_pred, verbose = verbose,\n",
        "                                  output_dict=output_frame,\n",
        "                                    label='Test Data' )\n",
        "\n",
        "  # Store results in a dataframe if ouput_frame is True\n",
        "  if output_frame:\n",
        "    results_df = pd.DataFrame([results_train,results_test])\n",
        "    # Set the label as the index\n",
        "    results_df = results_df.set_index('Label')\n",
        "    # Set index.name to none to get a cleaner looking result\n",
        "    results_df.index.name=None\n",
        "    # Return the dataframe\n",
        "    return results_df.round(3)"
      ],
      "metadata": {
        "id": "anDlGqL-lq_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of numeric columns and instantiate a StandardScaler\n",
        "num_cols = X_train.select_dtypes('number').columns\n",
        "scaler = StandardScaler()\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "# Construct the tuple for column transformer with the scaler\n",
        "num_pipe = make_pipeline(num_imputer,scaler)\n",
        "num_tuple = ('numeric',num_pipe, num_cols)\n",
        "num_tuple\n",
        "\n"
      ],
      "metadata": {
        "id": "j05BwmS1sVCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving list of categorical columns\n",
        "cat_cols = X_train.select_dtypes('object').columns\n",
        "# Constructing categorical preprocessing objects\n",
        "cat_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
        "ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "cat_pipe = make_pipeline(cat_imputer,ohe_encoder)\n",
        "cat_tuple = ('cat',cat_pipe, cat_cols)\n",
        "cat_tuple\n"
      ],
      "metadata": {
        "id": "LtREq-K1sz1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the preprocessor/ColumnTransformer\n",
        "preprocessor = ColumnTransformer([num_tuple, cat_tuple],\n",
        "                                 verbose_feature_names_out=False)\n",
        "preprocessor\n"
      ],
      "metadata": {
        "id": "T77rY-mKs3yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the preprocessor on training data\n",
        "preprocessor.fit(X_train)\n",
        "# Transform the training and test data\n",
        "X_train_tf = preprocessor.transform(X_train)\n",
        "X_test_tf = preprocessor.transform(X_test)\n",
        "#X_train_tf.head()\n"
      ],
      "metadata": {
        "id": "5AwckJB6s9H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg\n"
      ],
      "metadata": {
        "id": "qgJyKygPtIKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on the training data\n",
        "lin_reg.fit(X_train_tf, y_train)\n"
      ],
      "metadata": {
        "id": "O73U6QFjtPaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions for the training data\n",
        "y_predictions_train = lin_reg.predict(X_train_tf)\n",
        "# Get predictions for the testing data\n",
        "y_predictions_test = lin_reg.predict(X_test_tf)\n"
      ],
      "metadata": {
        "id": "EWAv3NoXtXah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_evaluation(lin_reg, X_train, y_train, X_test, y_test):\n",
        "    y_train_pred = lin_reg.predict(X_train_tf)\n",
        "    y_test_pred = lin_reg.predict(X_test_tf)\n",
        "\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    return r2_train, r2_test\n",
        "\n"
      ],
      "metadata": {
        "id": "AVlEV4MNygeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_r2, test_r2 = custom_evaluation(lin_reg, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"Training R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Compare the R-squared values to determine if the model is overfitting or underfitting\n",
        "if train_r2 > test_r2:\n",
        "    print(\"The model might be overfitting.\")\n",
        "elif train_r2 < test_r2:\n",
        "    print(\"The model might be underfitting.\")\n",
        "else:\n",
        "    print(\"The model's performance is balanced.\")\n"
      ],
      "metadata": {
        "id": "h3c6FJqNyrfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor  # Import RandomForestRegressor\n",
        "\n",
        "# Instantiate default random forest model\n",
        "rf = RandomForestRegressor(random_state = 42)\n",
        "# Model Pipeline\n",
        "rf_pipe = make_pipeline(preprocessor, rf)\n"
      ],
      "metadata": {
        "id": "TUT2C-hnyyM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model pipeline on the training data only\n",
        "rf_pipe.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "sTfPWmxz4Fqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use custom function to evaluate default model\n",
        "evaluate_regression(rf_pipe, X_train, y_train, X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "AZ0J4w6x4sBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yqAyYk0klfJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for tuning\n",
        "rf_pipe.get_params()\n",
        "\n"
      ],
      "metadata": {
        "id": "bD3lNDWV44NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define param grid with options to try\n",
        "params = {'randomforestregressor__max_depth': [None,10,15,20],\n",
        "          'randomforestregressor__n_estimators':[10,100,150,200],\n",
        "          'randomforestregressor__min_samples_leaf':[2,3,4],\n",
        "          'randomforestregressor__max_features':['sqrt','log2',None],\n",
        "          'randomforestregressor__oob_score':[True,False],\n",
        "          }\n"
      ],
      "metadata": {
        "id": "paiqdX4K9AJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV\n",
        "# Instantiate the gridsearch\n",
        "gridsearch = GridSearchCV(rf_pipe, params, n_jobs=-1, cv = 3, verbose=1)\n",
        "# Fit the gridsearch on training data\n",
        "gridsearch.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD_J5EvT9UK2",
        "outputId": "683f7541-deaf-4806-fe90-9f962f2986a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gridsearch.best_params_"
      ],
      "metadata": {
        "id": "vgZ6OWZl9b9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV\n",
        "\n",
        "# Define and refit best model\n",
        "best_rf = gridsearch.best_estimator_\n",
        "evaluate_regression(best_rf, X_train, y_train, X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "vQKG_ZwYPj3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}